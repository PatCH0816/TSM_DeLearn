{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "## MLP with TensorFlow 2\n",
    "The objective of the exercise is to implement computational graphs in TensorFlow 2.0 to train and use such an architecture. The constraints we put ourselves is to use **low-level** functions of TensorFlow, i.e. we will not use high-level functions to compose layers and to train the parameters.\n",
    "\n",
    "If you get this error in the execution of the first cell: ` ModuleNotFoundError: No module named 'tensorflow' `, it probably means TensorFlow 2.0 is not installed yet on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T19:00:21.720096Z",
     "start_time": "2021-04-07T19:00:21.388573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n",
      "MNIST data set ready. N=60000, D=784, n_classes=10\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# MNIST Dataset Preparation #\n",
    "#############################\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train_vec),(x_test, y_test_vec) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train_vec, 10, dtype='float64')\n",
    "y_test = tf.keras.utils.to_categorical(y_test_vec, 10, dtype='float64')\n",
    "N = x_train.shape[0]         # number of samples\n",
    "D = x_train.shape[1]         # dimension of input sample\n",
    "n_classes = y_train.shape[1] # output dim\n",
    "print('MNIST data set ready. N={}, D={}, n_classes={}'.format(N,D,n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T19:00:22.503571Z",
     "start_time": "2021-04-07T19:00:22.498979Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to sample a random batch from dataset\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0,len(data))  # create an array of index values\n",
    "    np.random.shuffle(idx)        # shuffle it\n",
    "    idx = idx[:num]               # take the first n indexes = size of batch\n",
    "    data_shuffle = data[idx]      # extract the batch using the random indexes\n",
    "    labels_shuffle = labels[idx]  # extract the labels using the random indexes\n",
    "\n",
    "    return data_shuffle, labels_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T19:32:40.201926Z",
     "start_time": "2021-04-07T19:28:03.416072Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function mlp_train at 0x0000020D812450D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: name 'fscope' is not defined\n",
      "WARNING: AutoGraph could not transform <function mlp_train at 0x0000020D812450D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: name 'fscope' is not defined\n",
      "epoch = 0, loss = 46.9905\n",
      "epoch = 1, loss = 41.6208\n",
      "epoch = 2, loss = 29.4246\n",
      "epoch = 3, loss = 20.0393\n",
      "epoch = 4, loss = 11.7576\n",
      "epoch = 5, loss = 5.1989\n",
      "epoch = 6, loss = 3.0188\n",
      "epoch = 7, loss = 2.5270\n",
      "epoch = 8, loss = 2.2359\n",
      "epoch = 9, loss = 2.0152\n",
      "epoch = 10, loss = 1.8419\n",
      "epoch = 11, loss = 1.7283\n",
      "epoch = 12, loss = 1.5201\n",
      "epoch = 13, loss = 1.4105\n",
      "epoch = 14, loss = 1.3244\n",
      "epoch = 15, loss = 1.3459\n",
      "epoch = 16, loss = 1.1903\n",
      "epoch = 17, loss = 1.1618\n",
      "epoch = 18, loss = 1.1366\n",
      "epoch = 19, loss = 1.0532\n",
      "epoch = 20, loss = 1.0225\n",
      "epoch = 21, loss = 0.9344\n",
      "epoch = 22, loss = 0.9072\n",
      "epoch = 23, loss = 0.9032\n",
      "epoch = 24, loss = 0.8702\n",
      "epoch = 25, loss = 0.8219\n",
      "epoch = 26, loss = 0.7893\n",
      "epoch = 27, loss = 0.7499\n",
      "epoch = 28, loss = 0.7194\n",
      "epoch = 29, loss = 0.7210\n",
      "epoch = 30, loss = 0.7258\n",
      "epoch = 31, loss = 0.7111\n",
      "epoch = 32, loss = 0.6567\n",
      "epoch = 33, loss = 0.6640\n",
      "epoch = 34, loss = 0.6157\n",
      "epoch = 35, loss = 0.6420\n",
      "epoch = 36, loss = 0.6091\n",
      "epoch = 37, loss = 0.5744\n",
      "epoch = 38, loss = 0.6072\n",
      "epoch = 39, loss = 0.5906\n",
      "epoch = 40, loss = 0.5938\n",
      "epoch = 41, loss = 0.6078\n",
      "epoch = 42, loss = 0.5863\n",
      "epoch = 43, loss = 0.5930\n",
      "epoch = 44, loss = 0.5824\n",
      "epoch = 45, loss = 0.5866\n",
      "epoch = 46, loss = 0.5445\n",
      "epoch = 47, loss = 0.5615\n",
      "epoch = 48, loss = 0.5594\n",
      "epoch = 49, loss = 0.5276\n",
      "Training took 460.69s\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Training phase #\n",
    "##################\n",
    "\n",
    "E = 50                # number of epochs\n",
    "B = 128               # batch size\n",
    "N = x_train.shape[0]  # number of samples\n",
    "D = x_train.shape[1]  # dimension of input sample\n",
    "H = 600               # number of neurons\n",
    "Amax = 5.0            # maximum learning rate alpha\n",
    "Amin = 0.01           # minimum learning rate alpha\n",
    "\n",
    "##############################################\n",
    "#  COMPLETE CODE BELOW WHERE YOU SEE # ...   #\n",
    "##############################################\n",
    "\n",
    "# Build the computational graph\n",
    "@tf.function # this decorator tells tf that a graph is defined\n",
    "def mlp_train(\n",
    "    x: tf.TensorSpec(shape=[None], dtype=tf.float64),\n",
    "    y: tf.TensorSpec(shape=[None], dtype=tf.float64),\n",
    "    alpha: tf.TensorSpec(shape=[None], dtype=tf.float64)\n",
    "):\n",
    "    # define nodes for forward computation for hidden neurons h and output neurons y_pred\n",
    "    \n",
    "    # h = ...  output of first layer after ReLu activation\n",
    "    # h = tf.maximum(tf.matmul(x, w1) + b1, 0)\n",
    "    h = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "    \n",
    "    # y_pred = ... output of second layer after sigmoid activation\n",
    "    # y_pred = 1 / (1 + tf.exp(-1 * (tf.matmul(h, w2) + b2, 0)))\n",
    "    y_pred = tf.sigmoid(tf.matmul(h, w2) + b2)\n",
    "    \n",
    "    # define nodes for forward computation for hidden neurons h and output neurons y_pred\n",
    "    diff = y_pred - y\n",
    "    loss = tf.reduce_mean(tf.pow(diff, 2))\n",
    "    \n",
    "    # define the gradients\n",
    "    grad_w1, grad_b1, grad_w2, grad_b2 = tf.gradients(loss, [w1, b1, w2, b2])\n",
    "    \n",
    "    # compute the new values of the gradients with the assign method (see slides)\n",
    "    w1.assign(w1 - alpha * grad_w1)\n",
    "    b1.assign(b1 - alpha * grad_b1)\n",
    "    w2.assign(w2 - alpha * grad_w2)\n",
    "    b2.assign(b2 - alpha * grad_b2)\n",
    "    return y_pred, loss\n",
    "\n",
    "# Init the tf.Variables w1, b1, w2, b2 following the given examples\n",
    "w1 = tf.Variable(tf.random.truncated_normal((D, H), stddev = 0.1, dtype='float64'))\n",
    "b1 = tf.Variable(tf.constant(0.0, shape=[H], dtype='float64'))\n",
    "w2 = tf.Variable(tf.random.truncated_normal((H, 10), stddev = 0.1, dtype='float64'))\n",
    "b2 = tf.Variable(tf.constant(0.0, shape=[10], dtype='float64'))\n",
    "\n",
    "# Run the computational graph\n",
    "J = [] # to store the evolution of loss J for each epoch\n",
    "learning_rate = np.linspace(Amax, Amin, E)\n",
    "start_time = time.time()\n",
    "for epoch in range(E):\n",
    "    J_epoch = 0.0\n",
    "    for _ in range(int(N/B)): # number of batches to visit for 1 epoch\n",
    "        # get batches calling the next_batch method provided above\n",
    "        x_train_batch, y_train_batch = next_batch(B, x_train, y_train)\n",
    "        with tf.device('/CPU:0'):  # change to /GPU:0 to move it to GPU\n",
    "            # call the graph with the batched input, target and alpha A\n",
    "            out = mlp_train(x_train_batch, y_train_batch, learning_rate[epoch])\n",
    "        y_pred, loss_val = out\n",
    "        J_epoch += loss_val\n",
    "    J.append(J_epoch)\n",
    "    print(\"epoch = {}, loss = {:.4f}\".format(epoch, J_epoch))\n",
    "print(\"Training took {:.2f}s\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T19:36:26.832841Z",
     "start_time": "2021-04-07T19:36:26.727153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20d81dd58c8>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the evolution of the loss\n",
    "plt.plot(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T19:36:28.413736Z",
     "start_time": "2021-04-07T19:36:28.113307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function mlp_test at 0x0000020DA44F3E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: name 'fscope' is not defined\n",
      "WARNING: AutoGraph could not transform <function mlp_test at 0x0000020DA44F3E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: name 'fscope' is not defined\n",
      "Forward pass on test set done.\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# Testing phase #\n",
    "#################\n",
    "\n",
    "N = x_test.shape[0]  # number of samples\n",
    "D = x_test.shape[1]  # dimension of input sample\n",
    "\n",
    "##############################################\n",
    "#  COMPLETE CODE BELOW WHERE YOU SEE # ...   #\n",
    "##############################################\n",
    "# Build the computational graph\n",
    "@tf.function # this decorator tells tf that a graph is defined\n",
    "def mlp_test(\n",
    "    x: tf.TensorSpec(shape=[None], dtype=tf.float64),\n",
    "    y: tf.TensorSpec(shape=[None], dtype=tf.float64)\n",
    "):\n",
    "    h = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "    y_pred = tf.sigmoid(tf.matmul(h, w2) + b2)\n",
    "    return y_pred\n",
    "\n",
    "# Run the computational graph\n",
    "with tf.device('/CPU:0'):  # change to /GPU:0 to move it to GPU\n",
    "    y_pred_test = mlp_test(x_test, y_test)\n",
    "\n",
    "print('Forward pass on test set done.')\n",
    "# At this stage, y_pred_test should contain the matrix of outputs on the test set with shape (N_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T19:36:34.238257Z",
     "start_time": "2021-04-07T19:36:34.231150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples  :  10000\n",
      "# correct  :  9830\n",
      "# missed   :  170\n",
      "accuracy   :  98.30 %\n",
      "error rate :  1.70 %\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy\n",
    "y_winner = np.argmax(y_pred_test, axis=1)\n",
    "N_test = y_winner.size\n",
    "num_correct = (y_winner == y_test_vec).sum()\n",
    "num_missed = N_test - num_correct\n",
    "accuracy = num_correct * 1.0 / N_test\n",
    "error_rate = num_missed * 1.0 / N_test\n",
    "print('# samples  : ', N_test)\n",
    "print('# correct  : ', num_correct)\n",
    "print('# missed   : ', num_missed)\n",
    "print('accuracy   :  %2.2f %%'% (accuracy*100.0))\n",
    "print('error rate :  %2.2f %%'% (error_rate*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "## MLP with TensorFlow 2 and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid and MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden-layer (Dense)         (None, 600)               471000    \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 10)                6010      \n",
      "=================================================================\n",
      "Total params: 477,010\n",
      "Trainable params: 477,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1201 - accuracy: 0.2660\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0886 - accuracy: 0.4232\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0837 - accuracy: 0.4927\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0796 - accuracy: 0.5290\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0754 - accuracy: 0.5589\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.0713 - accuracy: 0.5933\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0675 - accuracy: 0.6312\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0641 - accuracy: 0.6608\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0611 - accuracy: 0.6842\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.0583 - accuracy: 0.7027\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0559 - accuracy: 0.7204\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0537 - accuracy: 0.7360\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0517 - accuracy: 0.7527\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0499 - accuracy: 0.7693\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0483 - accuracy: 0.7827\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0468 - accuracy: 0.7946\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0454 - accuracy: 0.8032\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0441 - accuracy: 0.8119\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0429 - accuracy: 0.8189\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0418 - accuracy: 0.8244\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0408 - accuracy: 0.8286\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0398 - accuracy: 0.8314\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0390 - accuracy: 0.8359\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0382 - accuracy: 0.8384\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0374 - accuracy: 0.8411\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.0367 - accuracy: 0.8432\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0360 - accuracy: 0.8461\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0354 - accuracy: 0.8481\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0348 - accuracy: 0.8501\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0342 - accuracy: 0.8519\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0337 - accuracy: 0.8537\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0332 - accuracy: 0.8551\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0328 - accuracy: 0.8563\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0323 - accuracy: 0.8579\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0319 - accuracy: 0.8594\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0315 - accuracy: 0.8610\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0311 - accuracy: 0.8625\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0307 - accuracy: 0.8638\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0304 - accuracy: 0.8651\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0301 - accuracy: 0.8659\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0297 - accuracy: 0.8672\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0294 - accuracy: 0.8681\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0291 - accuracy: 0.8690\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.0288 - accuracy: 0.8700\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0286 - accuracy: 0.8710\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0283 - accuracy: 0.8722\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0280 - accuracy: 0.8731\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0278 - accuracy: 0.8739\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.0276 - accuracy: 0.8748\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0273 - accuracy: 0.8756\n",
      "10000/10000 - 1s - loss: 0.0262 - accuracy: 0.8862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.026188663809001447, 0.8862]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Direct from Slide 51\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Hidden layer - H neurons\n",
    "model.add(tf.keras.layers.Dense(H, input_shape=(D,), use_bias=True, activation='relu', name=\"hidden-layer\"))\n",
    "\n",
    "# Output layer - #classes\n",
    "model.add(tf.keras.layers.Dense(n_classes, input_shape=(H,), use_bias=True, activation='sigmoid', name=\"output-layer\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=Amin)\n",
    "model.compile(optimizer=sgd, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=B, epochs=E)\n",
    "\n",
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax and CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden-layer (Dense)         (None, 600)               471000    \n",
      "_________________________________________________________________\n",
      "output-layer (Dense)         (None, 10)                6010      \n",
      "=================================================================\n",
      "Total params: 477,010\n",
      "Trainable params: 477,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 1.0753 - accuracy: 0.7731\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.5134 - accuracy: 0.8743\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.4147 - accuracy: 0.8907\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.3696 - accuracy: 0.8990\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3416 - accuracy: 0.9063\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.3217 - accuracy: 0.9107\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.3061 - accuracy: 0.9151\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2931 - accuracy: 0.9183\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2819 - accuracy: 0.9213\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2722 - accuracy: 0.9238\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2635 - accuracy: 0.9264\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2554 - accuracy: 0.9284\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2481 - accuracy: 0.9306\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.2413 - accuracy: 0.9325\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.2349 - accuracy: 0.9344\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.2289 - accuracy: 0.9359\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.2233 - accuracy: 0.9376\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2179 - accuracy: 0.9394\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2129 - accuracy: 0.9409\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2081 - accuracy: 0.9420\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2033 - accuracy: 0.9432\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1990 - accuracy: 0.9448\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1947 - accuracy: 0.9457\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1907 - accuracy: 0.9469\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1867 - accuracy: 0.9480\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1829 - accuracy: 0.9490\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1793 - accuracy: 0.9500\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1759 - accuracy: 0.9511\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.1726 - accuracy: 0.9516\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1693 - accuracy: 0.9527\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1660 - accuracy: 0.9538\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1632 - accuracy: 0.9548\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1602 - accuracy: 0.9556\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1574 - accuracy: 0.9564\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1548 - accuracy: 0.9572\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.1521 - accuracy: 0.9577\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 42us/sample - loss: 0.1496 - accuracy: 0.9586\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.1471 - accuracy: 0.9591\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1448 - accuracy: 0.9604\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1425 - accuracy: 0.9609\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1402 - accuracy: 0.9617\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1381 - accuracy: 0.9622\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.1359 - accuracy: 0.9630\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.1339 - accuracy: 0.9638\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1319 - accuracy: 0.9639\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.1300 - accuracy: 0.9647\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1280 - accuracy: 0.9651\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1262 - accuracy: 0.9658\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1245 - accuracy: 0.9663\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1228 - accuracy: 0.9668\n",
      "10000/10000 - 1s - loss: 0.1285 - accuracy: 0.9625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12848244209513068, 0.9625]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Direct from Slide 51\n",
    "\n",
    "model1 = tf.keras.models.Sequential()\n",
    "\n",
    "# Hidden layer - H neurons\n",
    "model1.add(tf.keras.layers.Dense(H, input_shape=(D,), use_bias=True, activation='relu', name=\"hidden-layer\"))\n",
    "\n",
    "# Output layer - #classes\n",
    "model1.add(tf.keras.layers.Dense(n_classes, input_shape=(H,), use_bias=True, activation='softmax', name=\"output-layer\"))\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "model1.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history1 = model1.fit(x_train, y_train, batch_size=B, epochs=E)\n",
    "\n",
    "model1.evaluate(x_test, y_test, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
